{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6524d53e-b3d6-41f2-9d1e-c39f98d52e1d",
   "metadata": {},
   "source": [
    "# Dataframe from JSON\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "460e7835-3700-4d4a-a0aa-212e268c34c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "import pyspark.sql.functions as F\n",
    "import os\n",
    "import pandas as pd\n",
    "from pyspark import SparkConf, SparkContext\n",
    "from pyspark.sql import SparkSession, Row\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, MapType\n",
    "from pyspark.sql.functions import udf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "287badad-5bbd-4120-833e-372e4fffb5d3",
   "metadata": {},
   "source": [
    "## The 'pyarrow' lib provides a considerable performance improvement. But, it doesn't support ArrayType\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2e33eed2-c8f7-46f8-9943-cde7d53bc32a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/09/24 12:13:22 WARN Utils: Your hostname, eletricage resolves to a loopback address: 127.0.1.1; using 192.168.15.6 instead (on interface wlp4s0)\n",
      "23/09/24 12:13:22 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "WARNING: An illegal reflective access operation has occurred\n",
      "WARNING: Illegal reflective access by org.apache.spark.unsafe.Platform (file:/home/andre/Projects/Spark/spark-standalone/apps/venv/lib/python3.10/site-packages/pyspark/jars/spark-unsafe_2.12-3.2.4.jar) to constructor java.nio.DirectByteBuffer(long,int)\n",
      "WARNING: Please consider reporting this to the maintainers of org.apache.spark.unsafe.Platform\n",
      "WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations\n",
      "WARNING: All illegal access operations will be denied in a future release\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "23/09/24 12:13:23 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "spark = SparkSession.builder \\\n",
    ".config(\"spark.sql.execution.arrow.pyspark.enabled\", \"true\") \\\n",
    ".appName('test').master(\"spark://127.0.0.1:7077\")\\\n",
    ".getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f927ff89-14fd-4527-837c-29c8d3e2892e",
   "metadata": {},
   "source": [
    "The JSON file test.js\n",
    "\n",
    "```json\n",
    "[\n",
    "    {\n",
    "        \"name\": \"Andre\",\n",
    "        \"id\": 1,\n",
    "        \"doc_list\":[{\"docid\":\"DOC001\", \"name\":\"bla001.txt\"}, {\"docid\":\"DOC002\", \"name\":\"bla002.txt\"}],\n",
    "    },\n",
    "\n",
    "    {\n",
    "        \"name\": \"Noé\",\n",
    "        \"id\": 1,\n",
    "        \"doc_list\":[{\"docid\":\"DOC003\", \"name\":\"bla003.txt\"}, {\"docid\":\"DOC004\", \"name\":\"bla004.txt\"}],\n",
    "    }\n",
    "]\n",
    "\n",
    "\n",
    "```\n",
    "\n",
    "The easiest way to read a local file is import it using Pandas and convert it into a DataFrame object later.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcfe3e08-812a-49ec-9679-0716780b12f5",
   "metadata": {},
   "source": [
    "## The problem\n",
    "\n",
    "Besides to read the JSON file, off course, suppose that its desirable to extract the doc file names associated to the people's names. Note that for the each name there is a list of docs with 'docid' and 'name' belonged to the docs. How can we get a list of doc names for each person name in a new column called 'doc_names'?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4a72facd-aea4-432e-bde5-753caa4efaf9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/andre/Projects/Spark/spark-standalone/apps/venv/lib/python3.10/site-packages/pyspark/sql/pandas/conversion.py:329: UserWarning: createDataFrame attempted Arrow optimization because 'spark.sql.execution.arrow.pyspark.enabled' is set to true; however, failed by the reason below:\n",
      "  Unsupported type in conversion to Arrow: ArrayType(StructType(List(StructField(docid,StringType,true),StructField(name,StringType,true))),true)\n",
      "Attempting non-optimization as 'spark.sql.execution.arrow.pyspark.fallback.enabled' is set to true.\n",
      "  warnings.warn(msg)\n",
      "/home/andre/Projects/Spark/spark-standalone/apps/venv/lib/python3.10/site-packages/pyspark/sql/pandas/conversion.py:371: FutureWarning: iteritems is deprecated and will be removed in a future version. Use .items instead.\n",
      "  for column, series in pdf.iteritems():\n",
      "[Stage 1:===================>                                       (1 + 2) / 3]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---+------------------------------------------------------------------------------+\n",
      "|name |id |doc_list                                                                      |\n",
      "+-----+---+------------------------------------------------------------------------------+\n",
      "|Andre|1  |[{name -> bla001.txt, docid -> DOC001}, {name -> bla002.txt, docid -> DOC002}]|\n",
      "|Noé  |1  |[{name -> bla003.txt, docid -> DOC003}, {name -> bla004.txt, docid -> DOC004}]|\n",
      "+-----+---+------------------------------------------------------------------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Reading file using Pandas\n",
    "jdf = pd.read_json('test.js')\n",
    "# Converting to Spark dataframe\n",
    "sdf = spark.createDataFrame(jdf)\n",
    "# Showing the result\n",
    "sdf.show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6ba2ea4-eda9-4f53-924e-5f71e11f0899",
   "metadata": {},
   "source": [
    "User Defined Functions(UDF) is a way to parse information from a column. In this case, the docs inside the JSON file is available in a list of objects which is parsed by pySpark and convenient converted into Python data structure objects. In this case, a list of dictionaries which is eaiser to manipulate. Brilliant!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5a87b5f0-95cc-4d05-b742-a155f196a2aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "@udf\n",
    "def extract_doc_udf(data_list):\n",
    "    n = list()\n",
    "    for li in data_list:\n",
    "        \n",
    "        n += [v for k,v in li.items() if k == 'name']\n",
    "\n",
    "    return ', '.join(n)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "57fee3c9-fede-4649-a07e-9935bb82fd72",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---+----------------------+\n",
      "|name |id |doc_list              |\n",
      "+-----+---+----------------------+\n",
      "|Andre|1  |bla001.txt, bla002.txt|\n",
      "|Noé  |1  |bla003.txt, bla004.txt|\n",
      "+-----+---+----------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Running the UDF called 'extract_doc_udf' and storing into a new column called 'udf_res'\n",
    "dfu = sdf.withColumn('doc_list', extract_doc_udf(F.col('doc_list')))\n",
    "# Showing the result\n",
    "dfu.show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56069cfc-2d38-4ef9-848f-d7116afc7d3e",
   "metadata": {},
   "source": [
    "Alternativelly, it's possible to use a simple Python function passing the dataframe row as a parameter. But, to do that is necessary to use RDD framework instead UDF and then convert it to DataFrame object later. This way is useful when you parse different fields in a row in iteractive way. But, note that the performance will drop considerably depending on data amount."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "db3daafa-8e25-44d4-bb9a-4170e10c054b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_doc_rdd(row):\n",
    "    d = row.asDict()\n",
    "    n = list()\n",
    "    if 'doc_list' in d:\n",
    "        for li in d['doc_list']:\n",
    "            n += [v for k,v in li.items() if k == 'name']\n",
    "\n",
    "        d['doc_names'] = ', '.join(n)\n",
    "\n",
    "    return Row(**d)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8aba685e-ab0c-4b05-ab63-a2d9a97ee41f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---+------------------------------------------------------------------------------+----------------------+\n",
      "|name |id |doc_list                                                                      |doc_names             |\n",
      "+-----+---+------------------------------------------------------------------------------+----------------------+\n",
      "|Andre|1  |[{name -> bla001.txt, docid -> DOC001}, {name -> bla002.txt, docid -> DOC002}]|bla001.txt, bla002.txt|\n",
      "|Noé  |1  |[{name -> bla003.txt, docid -> DOC003}, {name -> bla004.txt, docid -> DOC004}]|bla003.txt, bla004.txt|\n",
      "+-----+---+------------------------------------------------------------------------------+----------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Executing 'extract_doc_rdd' using map method from rdd object\n",
    "rdd = sdf.rdd.map(extract_doc_rdd)\n",
    "# Converting into a dataframe object\n",
    "edf = rdd.toDF()\n",
    "# Showing the result\n",
    "edf.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dae46724-0644-48d4-bb52-87e951c3b9d0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
