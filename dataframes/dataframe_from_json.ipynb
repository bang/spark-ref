{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6524d53e-b3d6-41f2-9d1e-c39f98d52e1d",
   "metadata": {},
   "source": [
    "# Dataframe from JSON\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "460e7835-3700-4d4a-a0aa-212e268c34c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "import pyspark.sql.functions as F\n",
    "import pandas as pd\n",
    "import re\n",
    "from pyspark import SparkConf, SparkContext\n",
    "from pyspark.sql import SparkSession, Row\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, MapType, ArrayType\n",
    "from pyspark.sql.functions import udf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2e33eed2-c8f7-46f8-9943-cde7d53bc32a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting session from Spark\n",
    "spark = SparkSession.builder \\\n",
    ".appName('test').master('local[*]') \\\n",
    ".config(\"spark.cores.max\", \"2\") \\\n",
    ".config(\"spark.executor.memory\", \"2g\") \\\n",
    ".config(\"spark.sql.execution.arrow.pyspark.enabled\", \"true\") \\\n",
    ".config(\"spark.shuffle.service.enabled\", \"false\") \\\n",
    ".config(\"spark.dynamicAllocation.enabled\", \"true\") \\\n",
    ".getOrCreate()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f927ff89-14fd-4527-837c-29c8d3e2892e",
   "metadata": {},
   "source": [
    "### A proper JSON format in file dataframes/data/test.js\n",
    "\n",
    "```json\n",
    "[\n",
    "    {\n",
    "        \"name\": \"Andre\",\n",
    "        \"id\": 1,\n",
    "        \"doc_list\":[{\"docid\":\"DOC001\", \"name\":\"bla001.txt\"}, {\"docid\":\"DOC002\", \"name\":\"bla002.txt\"}],\n",
    "    },\n",
    "\n",
    "    {\n",
    "        \"name\": \"Noé\",\n",
    "        \"id\": 2,\n",
    "        \"doc_list\":[{\"docid\":\"DOC003\", \"name\":\"bla003.txt\"}, {\"docid\":\"DOC004\", \"name\":\"bla004.txt\"}],\n",
    "    }\n",
    "]\n",
    "\n",
    "\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcfe3e08-812a-49ec-9679-0716780b12f5",
   "metadata": {},
   "source": [
    "### Example problem\n",
    "\n",
    "Based on JSON structure in the file 'test.js'(shown above), extract the doc file names associated to the people's names as following: \n",
    "\n",
    "```text\n",
    "+-----+----------------------+\n",
    "|name |doc_names             |\n",
    "+-----+----------------------+\n",
    "|Andre|bla001.txt, bla002.txt|\n",
    "|Noé  |bla003.txt, bla004.txt|\n",
    "+-----+----------------------+\n",
    "```\n",
    "\n",
    "Note that, 'docid' is not desirable.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c287da29-db31-4a48-832c-c161072a2a46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------------------------------------------------------------------------------+\n",
      "|name |doc_list                                                                      |\n",
      "+-----+------------------------------------------------------------------------------+\n",
      "|Andre|[{docid -> DOC001, name -> bla001.txt}, {docid -> DOC002, name -> bla002.txt}]|\n",
      "|Noé  |[{docid -> DOC003, name -> bla003.txt}, {docid -> DOC004, name -> bla004.txt}]|\n",
      "+-----+------------------------------------------------------------------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "# Defining schema\n",
    "schema = StructType([\n",
    "    StructField(\"name\", StringType(), True),\n",
    "    StructField(\"doc_list\", ArrayType(MapType(StringType(),StringType(),True),True), True),\n",
    "])\n",
    "# Reading JSON file using Dataframe API setting 'multiline' option as true\n",
    "sdf = spark.read.option(\"multiline\", \"true\").json('data/test.json', schema=schema)\n",
    "sdf.show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11cc02e6-f597-4aff-bcb2-588fb94d2426",
   "metadata": {},
   "source": [
    "### Transformations steps\n",
    "\n",
    "1. Transform the list in 'doc_list' column into various rows using the `explode` pyspark function;\n",
    "2. For each of those rows, extract 'name' from the data structure using `getItem` function and drop the original column 'doc_ex';\n",
    "3. Transform doc names into a list againd group rows by 'name' column and using the `collect_list` as the aggregate function;\n",
    "4. Transform the list into a string separated by ',' using `concat_ws`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bf90af12-0f3c-4823-a576-359f66f2dc5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1\n",
      "+-----+------------------------------------------------------------------------------+-------------------------------------+\n",
      "|name |doc_list                                                                      |doc_ex                               |\n",
      "+-----+------------------------------------------------------------------------------+-------------------------------------+\n",
      "|Andre|[{docid -> DOC001, name -> bla001.txt}, {docid -> DOC002, name -> bla002.txt}]|{docid -> DOC001, name -> bla001.txt}|\n",
      "|Andre|[{docid -> DOC001, name -> bla001.txt}, {docid -> DOC002, name -> bla002.txt}]|{docid -> DOC002, name -> bla002.txt}|\n",
      "|Noé  |[{docid -> DOC003, name -> bla003.txt}, {docid -> DOC004, name -> bla004.txt}]|{docid -> DOC003, name -> bla003.txt}|\n",
      "|Noé  |[{docid -> DOC003, name -> bla003.txt}, {docid -> DOC004, name -> bla004.txt}]|{docid -> DOC004, name -> bla004.txt}|\n",
      "+-----+------------------------------------------------------------------------------+-------------------------------------+\n",
      "\n",
      "Step 2\n",
      "+-----+------------------------------------------------------------------------------+----------+\n",
      "|name |doc_list                                                                      |doc_name  |\n",
      "+-----+------------------------------------------------------------------------------+----------+\n",
      "|Andre|[{docid -> DOC001, name -> bla001.txt}, {docid -> DOC002, name -> bla002.txt}]|bla001.txt|\n",
      "|Andre|[{docid -> DOC001, name -> bla001.txt}, {docid -> DOC002, name -> bla002.txt}]|bla002.txt|\n",
      "|Noé  |[{docid -> DOC003, name -> bla003.txt}, {docid -> DOC004, name -> bla004.txt}]|bla003.txt|\n",
      "|Noé  |[{docid -> DOC003, name -> bla003.txt}, {docid -> DOC004, name -> bla004.txt}]|bla004.txt|\n",
      "+-----+------------------------------------------------------------------------------+----------+\n",
      "\n",
      "Step 3\n",
      "+-----+------------------------+\n",
      "|name |doc_list                |\n",
      "+-----+------------------------+\n",
      "|Noé  |[bla003.txt, bla004.txt]|\n",
      "|Andre|[bla001.txt, bla002.txt]|\n",
      "+-----+------------------------+\n",
      "\n",
      "Step 4\n",
      "+-----+---------------------+\n",
      "|name |doc_names            |\n",
      "+-----+---------------------+\n",
      "|Noé  |bla003.txt,bla004.txt|\n",
      "|Andre|bla001.txt,bla002.txt|\n",
      "+-----+---------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Step 1\n",
    "adf = sdf.withColumn(\"doc_ex\", F.explode(\"doc_list\"))\n",
    "print(\"Step 1\")\n",
    "adf.show(truncate=False)\n",
    "# Step 2 - Extracting the value of interest. In this case, the names of documents.\n",
    "adf = adf.withColumn(\"doc_name\", adf.doc_ex.getItem(\"name\")).drop(\"doc_ex\")\n",
    "print(\"Step 2\")\n",
    "adf.show(truncate=False)\n",
    "# Step 3 - Time to revert the 'explode' effect. For this, let's group rows by name and use 'collect_list' as aggregate function\n",
    "ndf = adf.groupBy(\"name\").agg(F.collect_list(\"doc_name\").alias('doc_list'))\n",
    "print(\"Step 3\")\n",
    "ndf.show(truncate=False)\n",
    "# Step 4 - Transforming this list into a string list separated by ',' character.\n",
    "ndf = ndf.withColumn(\"doc_names\", F.concat_ws(\",\", \"doc_list\")).drop(\"doc_list\")\n",
    "print(\"Step 4\")\n",
    "ndf.show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6ba2ea4-eda9-4f53-924e-5f71e11f0899",
   "metadata": {},
   "source": [
    "### UDF(Not recommended)\n",
    "User Defined Functions(UDF) can be a way to parse information from a column. In this case, the docs inside the JSON file is available in a list of objects which is parsed by pySpark and convenient converted into Python data structure objects which looks like more simpler to deal with. However, in this particularly scenario is not recommended because is possible to use spark functions which supports this operation offering a better optimization than UDF. Besides, in Python performance is not particularly good specially if you have neasted loops."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5a87b5f0-95cc-4d05-b742-a155f196a2aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "@udf\n",
    "def extract_doc(data_list: list) -> str:\n",
    "    n = list()\n",
    "    for li in data_list:\n",
    "        n += [v for k,v in li.items() if k == 'name']\n",
    "\n",
    "    return ','.join(n)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "57fee3c9-fede-4649-a07e-9935bb82fd72",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---------------------+\n",
      "|name |doc_names            |\n",
      "+-----+---------------------+\n",
      "|Andre|bla001.txt,bla002.txt|\n",
      "|Noé  |bla003.txt,bla004.txt|\n",
      "+-----+---------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "# Running the UDF called 'extract_doc_udf' and storing into a new column called 'udf_res'\n",
    "dfu = sdf.withColumn('doc_names', extract_doc(F.col('doc_list'))).select('name','doc_names')\n",
    "# Showing the result\n",
    "dfu.show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56069cfc-2d38-4ef9-848f-d7116afc7d3e",
   "metadata": {},
   "source": [
    "### \"UDF\" using RDD(less recommended)\n",
    "This is the old ways to handle UDFs. The reasons to not to do it is the same as the previous ways. And here is worse because you dealing directly with RDD and you will not have any optimization for doing that. If you don't know how to optimize RDD operations by yourself, don't use it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "db3daafa-8e25-44d4-bb9a-4170e10c054b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_doc_rdd(row):\n",
    "    d = row.asDict()\n",
    "    n = list()\n",
    "    if 'doc_list' in d:\n",
    "        for li in d['doc_list']:\n",
    "            n += [v for k,v in li.items() if k == 'name']\n",
    "\n",
    "        d['doc_names'] = ','.join(n)\n",
    "\n",
    "    return Row(**d)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8aba685e-ab0c-4b05-ab63-a2d9a97ee41f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---------------------+\n",
      "|name |doc_names            |\n",
      "+-----+---------------------+\n",
      "|Andre|bla001.txt,bla002.txt|\n",
      "|Noé  |bla003.txt,bla004.txt|\n",
      "+-----+---------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Executing 'extract_doc_rdd' using map method from rdd object\n",
    "rdd = sdf.rdd.map(extract_doc_rdd)\n",
    "# Converting into a dataframe object\n",
    "edf = rdd.toDF().select('name','doc_names')\n",
    "# Showing the result\n",
    "edf.show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d11b47f-4fc2-43a6-9433-712f7207deff",
   "metadata": {},
   "source": [
    "### Reading complex JSON using dynamic schema\n",
    "\n",
    "For the cases where JSON contains varying all the time. Example on dataframe/data/json-varying.csv:\n",
    "\n",
    "```json\n",
    "id,json_string\n",
    "1,'{\"name\": \"John Doe\", \"age\": 30}'\n",
    "2,'{\"city\": \"New York\", \"country\": \"USA\", \"zipcode\": \"10001\"}'\n",
    "3,'{\"product\": \"Laptop\", \"brand\": \"Dell\", \"specs\": {\"RAM\": \"16GB\", \"Storage\": \"512GB SSD\"}}'\n",
    "\n",
    "```\n",
    "\n",
    "Tipically this kind of data came from a column\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7c1dc01a-2f41-4ffe-9ba8-c172048f9fe3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----------------------------------------------------------------------------------------+\n",
      "|id |json_string                                                                             |\n",
      "+---+----------------------------------------------------------------------------------------+\n",
      "|1  |{\"name\": \"John Doe\", \"age\": 30}                                                         |\n",
      "|2  |{\"city\": \"New York\", \"country\": \"USA\", \"zipcode\": \"10001\"}                              |\n",
      "|3  |{\"product\": \"Laptop\", \"brand\": \"Dell\", \"specs\": {\"RAM\": \"16GB\", \"Storage\": \"512GB SSD\"}}|\n",
      "+---+----------------------------------------------------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Or from file.\n",
    "df = spark.read.csv('data/json-varying.csv', header=True, quote=\"'\")\n",
    "\n",
    "# If you're reading from a file, remove the quote char from string. Otherwise, the parser will not be able to return a object\n",
    "df = df.withColumn('json_string', F.regexp_replace('json_string', r\"\\'\", \"\"))\n",
    "df.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0f2bf586-4804-4732-a14a-19c6b5f7fa83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----------------------------------------------------------------------------------------+---------------------------------------------------------------+\n",
      "|id |json_string                                                                             |parsed                                                         |\n",
      "+---+----------------------------------------------------------------------------------------+---------------------------------------------------------------+\n",
      "|1  |{\"name\": \"John Doe\", \"age\": 30}                                                         |{30, null, null, null, John Doe, null, null, null}             |\n",
      "|2  |{\"city\": \"New York\", \"country\": \"USA\", \"zipcode\": \"10001\"}                              |{null, null, New York, USA, null, null, null, 10001}           |\n",
      "|3  |{\"product\": \"Laptop\", \"brand\": \"Dell\", \"specs\": {\"RAM\": \"16GB\", \"Storage\": \"512GB SSD\"}}|{null, Dell, null, null, null, Laptop, {16GB, 512GB SSD}, null}|\n",
      "+---+----------------------------------------------------------------------------------------+---------------------------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dynamic_schema = spark.read.json(df.rdd.map(lambda row: row['json_string'])).schema\n",
    "jdf = df.withColumn(\"parsed\", F.from_json('json_string', dynamic_schema))\n",
    "jdf.show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56e661c1-1d5e-4371-9f76-d8f9d39a73c4",
   "metadata": {},
   "source": [
    "# JSON from string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "3dab31a6-256d-4d3a-846c-086dfc0b58fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "json_str = \"\"\"[\n",
    "    {\n",
    "        \"name\":\"Andre\",\n",
    "        \"doc_list\":[{\"docid\":\"DOC001\", \"name\":\"bla001.txt\"}, {\"docid\":\"DOC002\", \"name\":\"bla002.txt\"}]\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"Noe\",\n",
    "        \"doc_list\":[{\"docid\":\"DOC002\", \"name\":\"bla002.txt\"}, {\"docid\":\"DOC003\", \"name\":\"bla003.txt\"}]\n",
    "    }\n",
    "]\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "e9c6ba84-4ddc-4e06-b283-195833e33e72",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------------------------------------------------------------------------------+\n",
      "|name |doc_list                                                                      |\n",
      "+-----+------------------------------------------------------------------------------+\n",
      "|Andre|[{docid -> DOC001, name -> bla001.txt}, {docid -> DOC002, name -> bla002.txt}]|\n",
      "|Noe  |[{docid -> DOC002, name -> bla002.txt}, {docid -> DOC003, name -> bla003.txt}]|\n",
      "+-----+------------------------------------------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "json_str=re.sub(r\"\\n\",\"\",json_str)\n",
    "sc = spark.sparkContext\n",
    "schema = StructType([\n",
    "    StructField(\"name\", StringType(), True),\n",
    "    StructField(\"doc_list\", ArrayType(MapType(StringType(), StringType())), True)\n",
    "])\n",
    "df = spark.read.json(sc.parallelize([json_str]), schema)\n",
    "df.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "3e1a9042-65c5-4aea-a1ff-cf58b629ecd0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---------------------+\n",
      "|name |doc_names            |\n",
      "+-----+---------------------+\n",
      "|Noe  |bla002.txt,bla003.txt|\n",
      "|Andre|bla001.txt,bla002.txt|\n",
      "+-----+---------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dfe = df.withColumn(\"item\", F.explode(\"doc_list\"))\n",
    "dfe = dfe.withColumn(\"doc\", dfe.item.getItem('name')).drop(\"item\") \\\n",
    "        .groupBy('name').agg(F.collect_list(\"doc\").alias(\"doclist\")) \\\n",
    "        .withColumn('doc_names', F.concat_ws(',','doclist')).drop(\"doclist\")\n",
    "dfe.show(truncate=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
