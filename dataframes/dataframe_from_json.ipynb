{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6524d53e-b3d6-41f2-9d1e-c39f98d52e1d",
   "metadata": {},
   "source": [
    "# Dataframe from JSON\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "460e7835-3700-4d4a-a0aa-212e268c34c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "import pyspark.sql.functions as F\n",
    "import pandas as pd\n",
    "import re\n",
    "from pyspark import SparkConf, SparkContext\n",
    "from pyspark.sql import SparkSession, Row\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, MapType, ArrayType\n",
    "from pyspark.sql.functions import udf\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "287badad-5bbd-4120-833e-372e4fffb5d3",
   "metadata": {},
   "source": [
    "\n",
    "The 'pyarrow' lib provides a considerable performance improvement. But, it doesn't support ArrayType\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2e33eed2-c8f7-46f8-9943-cde7d53bc32a",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder \\\n",
    ".config(\"spark.sql.execution.arrow.pyspark.enabled\", \"true\") \\\n",
    ".appName('test').master(\"spark://127.0.0.1:7077\")\\\n",
    ".getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f927ff89-14fd-4527-837c-29c8d3e2892e",
   "metadata": {},
   "source": [
    "The JSON file test.js\n",
    "\n",
    "```json\n",
    "[\n",
    "    {\n",
    "        \"name\": \"Andre\",\n",
    "        \"id\": 1,\n",
    "        \"doc_list\":[{\"docid\":\"DOC001\", \"name\":\"bla001.txt\"}, {\"docid\":\"DOC002\", \"name\":\"bla002.txt\"}],\n",
    "    },\n",
    "\n",
    "    {\n",
    "        \"name\": \"Noé\",\n",
    "        \"id\": 1,\n",
    "        \"doc_list\":[{\"docid\":\"DOC003\", \"name\":\"bla003.txt\"}, {\"docid\":\"DOC004\", \"name\":\"bla004.txt\"}],\n",
    "    }\n",
    "]\n",
    "\n",
    "\n",
    "```\n",
    "\n",
    "The easiest way to read a local file is import it using Pandas and convert it into a DataFrame object later.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcfe3e08-812a-49ec-9679-0716780b12f5",
   "metadata": {},
   "source": [
    "## The problem\n",
    "\n",
    "Besides to read the JSON file, off course, suppose that its desirable to extract the doc file names associated to the people's names. Note that for the each name there is a list of docs with 'docid' and 'name' belonged to the docs. How can we get a list of doc names for each person name in a new column called 'doc_names'? Example:\n",
    "\n",
    "```text\n",
    "+-----+----------------------+\n",
    "|name |doc_names             |\n",
    "+-----+----------------------+\n",
    "|Andre|bla001.txt, bla002.txt|\n",
    "|Noé  |bla003.txt, bla004.txt|\n",
    "+-----+----------------------+\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4a72facd-aea4-432e-bde5-753caa4efaf9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/andre/Projects/Spark/spark-standalone/apps/venv/lib/python3.10/site-packages/pyspark/sql/pandas/conversion.py:329: UserWarning: createDataFrame attempted Arrow optimization because 'spark.sql.execution.arrow.pyspark.enabled' is set to true; however, failed by the reason below:\n",
      "  Unsupported type in conversion to Arrow: ArrayType(StructType(List(StructField(docid,StringType,true),StructField(name,StringType,true))),true)\n",
      "Attempting non-optimization as 'spark.sql.execution.arrow.pyspark.fallback.enabled' is set to true.\n",
      "  warnings.warn(msg)\n",
      "/home/andre/Projects/Spark/spark-standalone/apps/venv/lib/python3.10/site-packages/pyspark/sql/pandas/conversion.py:371: FutureWarning: iteritems is deprecated and will be removed in a future version. Use .items instead.\n",
      "  for column, series in pdf.iteritems():\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------------------------------------------------------------------------------+\n",
      "|name |doc_list                                                                      |\n",
      "+-----+------------------------------------------------------------------------------+\n",
      "|Andre|[{name -> bla001.txt, docid -> DOC001}, {name -> bla002.txt, docid -> DOC002}]|\n",
      "|Noé  |[{name -> bla003.txt, docid -> DOC003}, {name -> bla004.txt, docid -> DOC004}]|\n",
      "+-----+------------------------------------------------------------------------------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "StructType(List(StructField(name,StringType,true),StructField(doc_list,ArrayType(MapType(StringType,StringType,true),true),true)))"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Reading file using Pandas. Not ideal, but I'm having problems to read from local files yet. But, I'll fix that some day\n",
    "pdf = pd.read_json('test.json')\n",
    "# Converting to Spark dataframe\n",
    "sdf = spark.createDataFrame(pdf).drop(\"id\")\n",
    "# Showing the result\n",
    "sdf.show(truncate=False)\n",
    "sdf.schema"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11cc02e6-f597-4aff-bcb2-588fb94d2426",
   "metadata": {},
   "source": [
    "# DataFrame\n",
    "Probably, the best way to do this once DataFrame API has been enreached with optimizations for the last years. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "bf90af12-0f3c-4823-a576-359f66f2dc5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------------------------------------------------------------------------------+----------+\n",
      "|name |doc_list                                                                      |doc_name  |\n",
      "+-----+------------------------------------------------------------------------------+----------+\n",
      "|Andre|[{name -> bla001.txt, docid -> DOC001}, {name -> bla002.txt, docid -> DOC002}]|bla001.txt|\n",
      "|Andre|[{name -> bla001.txt, docid -> DOC001}, {name -> bla002.txt, docid -> DOC002}]|bla002.txt|\n",
      "|Noé  |[{name -> bla003.txt, docid -> DOC003}, {name -> bla004.txt, docid -> DOC004}]|bla003.txt|\n",
      "|Noé  |[{name -> bla003.txt, docid -> DOC003}, {name -> bla004.txt, docid -> DOC004}]|bla004.txt|\n",
      "+-----+------------------------------------------------------------------------------+----------+\n",
      "\n",
      "+-----+---------------------+\n",
      "|name |doc_names            |\n",
      "+-----+---------------------+\n",
      "|Andre|bla001.txt,bla002.txt|\n",
      "|Noé  |bla003.txt,bla004.txt|\n",
      "+-----+---------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Work with single lines is better in this case. Explode will make each item from the list a row in DF\n",
    "adf = sdf.withColumn(\"doc_ex\", F.explode(\"doc_list\"))\n",
    "# Extracting the value of interest. In this case, 'name'(doc names)\n",
    "adf = adf.withColumn(\"doc_name\", adf.doc_ex.getItem(\"name\")).drop(\"doc_ex\")\n",
    "adf.show(truncate=False)\n",
    "# Time to revert the 'explode' effect. For this, let's group rows by name and use 'collect_list' as aggregate function\n",
    "# in order to have a list again. But now, only with doc names.\n",
    "ndf = adf.groupBy(\"name\").agg(F.collect_list(\"doc_name\").alias('doc_list'))\n",
    "# Now we need only transform this list \n",
    "ndf = ndf.withColumn(\"doc_names\", F.concat_ws(\",\", \"doc_list\")).drop(\"doc_list\")\n",
    "ndf.show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6ba2ea4-eda9-4f53-924e-5f71e11f0899",
   "metadata": {},
   "source": [
    "# UDF(Not recommended)\n",
    "User Defined Functions(UDF) is a way to parse information from a column. In this case, the docs inside the JSON file is available in a list of objects which is parsed by pySpark and convenient converted into Python data structure objects. In this case, a list of dictionaries which is eaiser to manipulate. Brilliant!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5a87b5f0-95cc-4d05-b742-a155f196a2aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "@udf\n",
    "def extract_doc(data_list):\n",
    "    n = list()\n",
    "    for li in data_list:\n",
    "        n += [v for k,v in li.items() if k == 'name']\n",
    "\n",
    "    return ','.join(n)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "57fee3c9-fede-4649-a07e-9935bb82fd72",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---------------------+\n",
      "|name |doc_names            |\n",
      "+-----+---------------------+\n",
      "|Andre|bla001.txt,bla002.txt|\n",
      "|Noé  |bla003.txt,bla004.txt|\n",
      "+-----+---------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Running the UDF called 'extract_doc_udf' and storing into a new column called 'udf_res'\n",
    "dfu = sdf.withColumn('doc_names', extract_doc(F.col('doc_list'))).select('name','doc_names')\n",
    "# Showing the result\n",
    "dfu.show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56069cfc-2d38-4ef9-848f-d7116afc7d3e",
   "metadata": {},
   "source": [
    "# \"UDF\" using RDD(less recommended)\n",
    "Alternativelly, it's possible to use a simple Python function passing the dataframe row as a parameter. But, to do that is necessary to use RDD framework instead UDF and then convert it to DataFrame object later. This way is useful when you parse different fields in a row in iteractive way. But, note that the performance will drop considerably depending on data amount."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "db3daafa-8e25-44d4-bb9a-4170e10c054b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_doc_rdd(row):\n",
    "    d = row.asDict()\n",
    "    n = list()\n",
    "    if 'doc_list' in d:\n",
    "        for li in d['doc_list']:\n",
    "            n += [v for k,v in li.items() if k == 'name']\n",
    "\n",
    "        d['doc_names'] = ','.join(n)\n",
    "\n",
    "    return Row(**d)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8aba685e-ab0c-4b05-ab63-a2d9a97ee41f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---------------------+\n",
      "|name |doc_names            |\n",
      "+-----+---------------------+\n",
      "|Andre|bla001.txt,bla002.txt|\n",
      "|Noé  |bla003.txt,bla004.txt|\n",
      "+-----+---------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Executing 'extract_doc_rdd' using map method from rdd object\n",
    "rdd = sdf.rdd.map(extract_doc_rdd)\n",
    "# Converting into a dataframe object\n",
    "edf = rdd.toDF().select('name','doc_names')\n",
    "# Showing the result\n",
    "edf.show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56e661c1-1d5e-4371-9f76-d8f9d39a73c4",
   "metadata": {},
   "source": [
    "# JSON from string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3dab31a6-256d-4d3a-846c-086dfc0b58fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "json_str = \"\"\"[\n",
    "    {\n",
    "        \"name\":\"Andre\",\n",
    "        \"doc_list\":[{\"docid\":\"DOC001\", \"name\":\"bla001.txt\"}, {\"docid\":\"DOC002\", \"name\":\"bla002.txt\"}]\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"Noe\",\n",
    "        \"doc_list\":[{\"docid\":\"DOC002\", \"name\":\"bla002.txt\"}, {\"docid\":\"DOC003\", \"name\":\"bla003.txt\"}]\n",
    "    }\n",
    "]\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e9c6ba84-4ddc-4e06-b283-195833e33e72",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------------------------------------------------------------------------------+\n",
      "|name |doc_list                                                                      |\n",
      "+-----+------------------------------------------------------------------------------+\n",
      "|Andre|[{docid -> DOC001, name -> bla001.txt}, {docid -> DOC002, name -> bla002.txt}]|\n",
      "|Noe  |[{docid -> DOC002, name -> bla002.txt}, {docid -> DOC003, name -> bla003.txt}]|\n",
      "+-----+------------------------------------------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "json_str=re.sub(r\"\\n\",\"\",json_str)\n",
    "sc = spark.sparkContext\n",
    "schema = StructType([\n",
    "    StructField(\"name\", StringType(), True),\n",
    "    StructField(\"doc_list\", ArrayType(MapType(StringType(), StringType())), True)\n",
    "])\n",
    "df = spark.read.json(sc.parallelize([json_str]), schema)\n",
    "df.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "3e1a9042-65c5-4aea-a1ff-cf58b629ecd0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---------------------+\n",
      "|name |doc_names            |\n",
      "+-----+---------------------+\n",
      "|Noe  |bla002.txt,bla003.txt|\n",
      "|Andre|bla001.txt,bla002.txt|\n",
      "+-----+---------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dfe = df.withColumn(\"item\", F.explode(\"doc_list\"))\n",
    "dfe = dfe.withColumn(\"doc\", dfe.item.getItem('name')).drop(\"item\") \\\n",
    "        .groupBy('name').agg(F.collect_list(\"doc\").alias(\"doclist\")) \\\n",
    "        .withColumn('doc_names', F.concat_ws(',','doclist')).drop(\"doclist\")\n",
    "dfe.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "bcfa7071-d414-405d-98db-a4a89e93b6c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "json_str2=\"\"\"[\n",
    "        {\n",
    "        \"name\": \"Andre\",\n",
    "        \"id\": 1,\n",
    "        \"l1\":[{\"a\":1},{\"b\":2}]\n",
    "        },\n",
    "        {\n",
    "        \"name\": \"Noé\",\n",
    "        \"id\": 2,\n",
    "        \"l1\":[{\"c\":3},{\"d\":4}]\n",
    "        }\n",
    "]\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "10059698-be55-483f-908f-def49a98be70",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+--------------------+\n",
      "|id |name |l1                  |\n",
      "+---+-----+--------------------+\n",
      "|1  |Andre|[{a -> 1}, {b -> 2}]|\n",
      "|2  |Noé  |[{c -> 3}, {d -> 4}]|\n",
      "+---+-----+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "json_str=re.sub(r\"\\n\",\"\",json_str2)\n",
    "sc = spark.sparkContext\n",
    "schema = StructType([\n",
    "    StructField(\"id\", StringType(), True),\n",
    "    StructField(\"name\", StringType(), True),\n",
    "    StructField(\"l1\", ArrayType(MapType(StringType(), StringType())), True)\n",
    "])\n",
    "df2 = spark.read.json(sc.parallelize([json_str]), schema)\n",
    "df2.show(truncate=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
